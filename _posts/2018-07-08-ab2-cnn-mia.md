---
title:  "[Reading] CNN architectures in medical imaging analysis"
date:   2018-07-08 10:00:00
comments: true
excerpt: "Read papers on Medical Imaging Analysis with CNN."
tags:
  - annotated bibliography
---



1. Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. ArXiv:1505.04597 [Cs]. Retrieved from http://arxiv.org/abs/1505.04597


    **U-Net; Segmentation**

    The task of medical imaging localization is to identify lesions' location, i.e.: classify each pixel to be the lesion or not. The previous sliding-window approach is usually computational slow and redundant, because divides the whole image into small patches and runs each patch separately through the CNN. It also through away the context information. U-net built upon the fully convolutional network (fCNN). With the fCNN, it outputs a segmentation map. The model architecture consists of a contracting path (downsampling) to capture context, and a symmetric expanding path (upsampling) for precise localization. Therefore it yields a u-shaped architecture. The upsampling part also concatenates the corresponding part from the downsampling with the skip-connection. The results showed it preforms significantly better than the sliding-window convolutional network on different biomedical segmentation tasks.

    ![U-Net architecture](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)

1. F. Milletari, N. Navab, and S.-A. Ahmadi, “V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation,” arXiv:1606.04797 [cs], Jun. 2016.

    **V-Net, 3D volumetric segmentation**

    This work aims to build an end-to-end fully convolutional network for 3D volumetric medical image segmentation. The previous patch-wise classification approach only considered the local context and discard the global information. Different from previous approaches that deals with 3D input volumes slide-wise, the authors propose to use volumetric convolutions instead.


    ![V-Net architecture](https://vitalab.github.io/deep-learning/images/vnet/vnet.png)

     The overall architecture is similar to U-Net, which consists of a compression and decompression path. The 2D convolutions are replaced by 3D convolutions using volumetric kernels. The authors did not use any pooling layers, instead, they use non-overlapping 2 x 2 x 2 convolutional volume patches so that the resulting feature maps is halved. The rationale is to save memory footprint during training. The author also proposes a novel objective function based on Dice coefficient maximization. The author demonstrates fast and accurate results on prostate MRI test volumes.



1. Kermany, D. S., Goldbaum, M., Cai, W., Valentim, C. C. S., Liang, H., Baxter, S. L., … Zhang, K. (2018). Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning. Cell, 172(5), 1122-1131.e9. https://doi.org/10.1016/j.cell.2018.02.010

    **classfication**

    This work applied transfer learning to classify macular degeneration and diabetic retinopathy using retinal optical coherence tomography (OCT) images. They used an Inception V3 architecture pretrained on ImageNet. The pretrained layer was frozen and connected with fully-connected layers. The author found fine-tuning the pretrained layers using backpropagation tended to decrease model performance due to overfitting. For the multi-class classification (4 classes, normal, two urgent referrals, and one routine referral), the model achieved an accuracy of 96.6%, with a sensitivity of 97.8%, a specificity of 97.4%, and a weighted error of 6.6%. This work is mainly conducted by clinicians, the main contribution is to collect a multi-center large dataset with disease Classification annotations.


1. M. Havaei, N. Guizard, N. Chapados, and Y. Bengio, “HeMIS: Hetero-Modal Image Segmentation,” arXiv:1607.05194 [cs], Jul. 2016.

    **missing imaging modalities; segmentation**

    One challenge of the medical imaging analysis is the missing modalities. For example, the MRI imaging has different modalities such as T1, T2, T2 FLAIR, DWI. A typical task usually requires several modalities to be available. In clinical settings, however, the missing modalities are very common, since doctors will prescribe MRI according to the specific diseases. Two kinds of approaches exist to deal with the missing modalities: 1) Data synthesize; 2) a robust model to inference without the missing modalities. The paper focuses on the 2nd approach.

    It proposes an end-to-end deep learning framework (HeMIS, Hetero-Modal Image Segmentation architecture) that can segment medical images from incomplete multi-modal datasets. The framework consists of three parts:

    ![Hetero-Modal Image Segmentation architecture]({{ "/assets/images/HeMIS.gif" | absolute_url }})

    1. The back end: Each imaging modality goes through a separate convolutional pipeline independently to generate a feature map. This maps each modality into an embedding common to all modalities, within which vector algebra operations carry well-defined semantics (in part 2).

    2. The abstraction layers: For the available features maps extracted from part 1, the overall mean and variance is calculated as modality fusion.

    3. The front end: In combined the merged modalities in part 2 to go through another conv and softmax layer to produce the final segmentation output.

    The objective function is a pixelwise class cross-entropy loss. To train the network, the author applied pseudo-curriculum training, where the model starts learning from easy scenarios before turning to more difficult ones. Here the authors first trained the fCNN with all modalities, then randomly dropping one or several modalities to make the model more robust.



1. A. Chartsias, T. Joyce, M. V. Giuffrida, and S. A. Tsaftaris, “Multimodal MR Synthesis via Modality-Invariant Latent Representation,” IEEE Transactions on Medical Imaging, vol. 37, no. 3, pp. 803–814, Mar. 2018.

    **missing imaging modalities; MRI synthesis**

    As stated in the above paper, there are two main approaches to deal with the incomplete imaging modalities. This paper focuses on data synthesis to impute missing images. The authors proposed a deep fCNN with multi-input and multi-output for MR synthesis. This overcomes the previous image synthesis that only learns mappings *between pairs* of image modalities.

    ![MR_Synthesis]({{ "/assets/images/MR_Synthesis.gif" | absolute_url }})

    The fCNN network takes aligned images with different modalities as input, and allows users to an arbitrary combination of modalities as output images. Its structure is a little similar to the previous paper. It composes of three stages: encoding, representation fusion, and decoding. In the encoder part, all inputs are projected into a shared latent representation space via a small U-Net architecture. In the fusion part, the pixel-wise max function fuses the latent representations into a single representation. And in the decoder part, a shallower fCNN maps the fused representation to the required output modality.

    The challenges for MRI synthesis is to build a model that can take as any subset of the input modalities to produce its output. Simply embedding inputs into the same representation space does not ensure that they share a meaningful latent representation, i.e. the meaning of the latent representation is dependent on its original modality. The magic happened at the cost function. The author produces a latent representation that is *independent* of the originating modality.

    The cost function is the mathematical expression of the following three goals:

    1) Each modality's individual latent representation should produce all output as accurately as possible.

    $$\begin{equation} c_{1}(k|\boldsymbol \theta,\boldsymbol \psi) = \frac {1}{m}\sum _{i=1}^{n} \sum _{j=1}^{m} MAE(g(f(X^{k}_{i}|\theta _{i})|\psi _{j}),Y^{k}_{j}) \end{equation}$$


    It can be seen as the sum of each input modality’s average reconstruction error across all outputs.

    2) The latent representations from all input modalities should be close in the Euclidean sense.

    $$\begin{align} c_{2}(k|\boldsymbol \theta) = \frac {1}{|C||P|} \sum _{c \in C} \sum _{p\in P} var(f(X^{k}_{1}|\theta _{1})_{p,c},\ldots, f(X^{k}_{n}|\theta _{n})_{p,c})\!\!\!\notag \!\!\!\\ {}\end{align}$$


    3) The fused latent representation should produce all outputs as accurately as possible.

    $$ \begin{align}&\hspace {-2pc}c_{3}(k|\boldsymbol \theta,\boldsymbol \psi) \notag \\=&\frac {1}{m}\sum _{j=1}^{m} MAE(g(\alpha (f(X^{k}_{1}|\theta _{1}), \ldots, f(X^{k}_{n}|\theta _{n}))|\psi _{j}), Y^{k}_{j})\notag \\ {}\end{align} $$

    The model is evaluated on the ISLES and BRATS datasets and demonstrate statistically significant improvements over state-of-the-art methods for single input tasks.


1. A. Esteva et al., “Dermatologist-level classification of skin cancer with deep neural networks,” Nature, vol. 542, no. 7639, pp. 115–118, 02 2017.

    **dermatology, transfer learning**

    This is one of the early application and influencial work in medical image analysis with deep learning. The model is built upon Inception v3 CNN architecture with transfer learning to readopt the model to skin lesion classification. It used 0.1 M clinical images with over two thousand diseases. The authors compared the model's performance with 21 dermatologists and showed it achieves performance on par with all tested doctors. However, the ROC curve problematically compares the discrete judgment of doctors for each image, with the continuous probability outputted by the CNN model. To verify the assersion that "AI surpasses doctors' performances", the work will need to provide more solid and equal comparision with statistical significance.  




1. J. Kawahara and G. Hamarneh, “Fully Convolutional Neural Networks to Detect Clinical Dermoscopic Features,” p. 8, 2018.

    **dermatology, superpixel classification, segmentation**

    This paper aims to classify clinical dermoscopic features within superpixels. It reformulates the superpixel classification task as a semantic segmentation problem. The CNN architecture built upon VGG16 network. The authors resize selected responses/feature maps throughout the network to match the size of the input image using bilinear interpolation. These selected resized feature maps are concatenated, allowing to directly consider feature maps from several network layers. This design is motivated by the authors' observation that the appearance of dermoscopic features are subtle, and may be represented in shallower layers with high spatial resolutions. The authors then use 1 x 1 convolution to reduce the number of parameters. To deal with pixel-, class- and sample-imbalance, the author used a negative multi-label Sorensen-Dice-F1 loss function. The work achieved 1st place in the 2017 ISIC-ISBI part 2: Dermoscopic Feature Classification challenge.


1. J. Kawahara, S. Daneshvar, G. Argenziano, and G. Hamarneh, “7-Point Checklist and Skin Lesion Classification using Multi-Task Multi-Modal Neural Nets,” IEEE Journal of Biomedical and Health Informatics, pp. 1–1, 2018.

    **dermatology, multi-task, multi-modal**

    To distinguish between benign and malignant skin tumors, dermatologists use rule-based diagnostic checklists to facilitate diagnosis. This paper aims to classify the 7-point melanoma checklist criteria and perform skin lesion diagnosis with a deep convolutional neural network. To enable the network utilizes the multi-modal data(clinical and dermoscopic images, and patient meta-data), the authors designed a multi-modal multi-task loss function that considers different combinations of the input modalities. This makes the model robust to missing data at inference time. The experiment results are benchmarked along with the dataset made available.  


1. Z. Li et al., “Thoracic Disease Identification and Localization with Limited Supervision,” arXiv:1711.06373 [cs, stat], Nov. 2017.

    **Classification and localization**

    Manually labeling of medical images, especially the ones with localization, is costly to acquire. This work aims to classify and localize the disease within one model. It adopts the idea of handing an image as a group of grid cells, and treat each patch as a classification target (based on ideas of YOLO, SSD, MIL). For the model architecture, the inputs consist of two kinds of chest X-ray images: image with and without bounding boxes. The bounding box information is encoded with the patch disease classification, where if a patch is covered by the bounding box, its $k$th disease classification is 1. The input image first goes through the ResNet to encode into a set of abstracted feature maps. Then the feature map is up- or down-sampled to a $P x P$ patch grid. Then it goes through two convolution layers to generate the output tensor $P x P x K* for $K$ disease label prediction of the $P x P$ patches.

    The model designed two loss functions for input images with or without ground-truth bounding boxes. It defines the image-level probabilities given the image disease label with or without the bounding-box information. The probability is the multiplication of each individual grid's probability for a disease label. The combined loss function is the cross-entropy loss of the weighted sum the image-level probabilities in two cases. The proposed model achieves significant accuracy improvement over the state-of-the-art approaches on classification and localization.


    [//]: <> (self-transfer learning, transfer the knowledge of classification to localization.)

1. T. Würfl et al., “Deep Learning Computed Tomography: Learning Projection-Domain Weights From Image Domain in Limited Angle Problems,” IEEE Transactions on Medical Imaging, vol. 37, no. 6, pp. 1454–1463, Jun. 2018.

    **Reconstruction, CT**

    This is an interesting work that combines the traditional image processing pipeline with learning-based approach. It maps an analytic reconstruction pipeline to a neural network as a one-to-one correspondence.
