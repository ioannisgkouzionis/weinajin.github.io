---
title:  "[Reading] CNN architectures in medical imaging analysis"
date:   2018-07-08 10:00:00
comments: true
#excerpt: ". "
tags:
  - annotated bibliography
---



1. Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. ArXiv:1505.04597 [Cs]. Retrieved from http://arxiv.org/abs/1505.04597


    **U-Net; Segmentation**

    The task of medical imaging localization is to identify lesions' location, i.e.: classify each pixel to be the lesion or not. The previous sliding-window approach is usually computational slow and redundant, because divides the whole image into small patches and runs each patch separately through the CNN. It also through away the context information. U-net built upon the fully convolutional network (fCNN). With the fCNN, it outputs a segmentation map. The model architecture consists of a contracting path (downsampling) to capture context, and a symmetric expanding path (upsampling) for precise localization. Therefore it yields a u-shaped architecture. The upsampling part also concatenates the corresponding part from the downsampling with the skip-connection. The results showed it preforms significantly better than the sliding-window convolutional network on different biomedical segmentation tasks.

    ![U-Net architecture](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)



1. Kermany, D. S., Goldbaum, M., Cai, W., Valentim, C. C. S., Liang, H., Baxter, S. L., … Zhang, K. (2018). Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning. Cell, 172(5), 1122-1131.e9. https://doi.org/10.1016/j.cell.2018.02.010

    **classfication**

    This work applied transfer learning to classify macular degeneration and diabetic retinopathy using retinal optical coherence tomography (OCT) images. They used an Inception V3 architecture pretrained on ImageNet. The pretrained layer was frozen and connected with fully-connected layers. The author found fine-tuning the pretrained layers using backpropagation tended to decrease model performance due to overfitting. For the multi-class classification (4 classes, normal, two urgent referrals, and one routine referral), the model achieved an accuracy of 96.6%, with a sensitivity of 97.8%, a specificity of 97.4%, and a weighted error of 6.6%. This work is mainly conducted by clinicians, the main contribution is to collect a multi-center large dataset with disease Classification annotations.


1. M. Havaei, N. Guizard, N. Chapados, and Y. Bengio, “HeMIS: Hetero-Modal Image Segmentation,” arXiv:1607.05194 [cs], Jul. 2016.

    **missing imaging modalities; segmentation**

    One challenge of the medical imaging analysis is the missing modalities. For example, the MRI imaging has different modalities such as T1, T2, T2 FLAIR, DWI. A typical task usually requires several modalities to be available. In clinical settings, however, the missing modalities are very common, since doctors will prescribe MRI according to the specific diseases. Two kinds of approaches exist to deal with the missing modalities: 1) Data synthesize; 2) a robust model to inference without the missing modalities. The paper focuses on the 2nd approach.

    It proposes an end-to-end deep learning framework (HeMIS, Hetero-Modal Image Segmentation architecture) that can segment medical images from incomplete multi-modal datasets. The framework consists of three parts:

    ![Hetero-Modal Image Segmentation architecture]({{ "/assets/images/HeMIS.gif" | absolute_url }})

    1. The back end: Each imaging modality goes through a separate convolutional pipeline independently to generate a feature map. This maps each modality into an embedding common to all modalities, within which vector algebra operations carry well-defined semantics (in part 2).

    2. The abstraction layers: For the available features maps extracted from part 1, the overall mean and variance is calculated as modality fusion.

    3. The front end: In combined the merged modalities in part 2 to go through another conv and softmax layer to produce the final segmentation output.

    The objective function is a pixelwise class cross-entropy loss. To train the network, the author applied pseudo-curriculum training, where the model starts learning from easy scenarios before turning to more difficult ones. Here the authors first trained the fCNN with all modalities, then randomly dropping one or several modalities to make the model more robust.


1. A. Chartsias, T. Joyce, M. V. Giuffrida, and S. A. Tsaftaris, “Multimodal MR Synthesis via Modality-Invariant Latent Representation,” IEEE Transactions on Medical Imaging, vol. 37, no. 3, pp. 803–814, Mar. 2018.

  ***missing imaging modalities; MRI synthesis**

  As stated in the above paper, there are two main approaches to deal with the incomplete imaging modalities. This paper focuses on data synthesis to impute missing images. The authors proposed a deep fCNN with multi-input and multi-output for MR synthesis. This overcomes the previous image synthesis that only learns mappings *between pairs* of image modalities.

  ![MR_Synthesis]({{ "/assets/images/MR_Synthesis.gif" | absolute_url }})

  The fCNN network takes aligned images with different modalities as input, and allows users to a arbitrary combination of modalities as output images. Its structure is a little similar to the previous paper. It composes of three stages: encoding, representation fusion, and decoding. In the encoder part, all inputs are projected into a shared latent representation space via a small U-Net architecture. In the fusion part, the pixel-wise max function fuses the latent representations into a single representation. And in the decoder part, a shallower fCNN maps the fused representation to the required output modality.

  The challenges for MRI synthesis is to build a model that can take as any subset of the input modalities to produce its output. Simply embedding inputs into the same representation space does not ensure that they share a meaningful latent representation, i.e. the meaning of the latent representation is depentdent on its original modality. The magic happened at the cost function. The author produces a latent representation that is *independent* of the originating modality.

  The cost function is the mathematical expression of the following three goals:
  1. Each modality's individual latent representation should produce all output as accurately as possible.

  \begin{equation} c_{1}(k|\boldsymbol \theta,\boldsymbol \psi) = \frac {1}{m}\sum _{i=1}^{n} \sum _{j=1}^{m} MAE(g(f(X^{k}_{i}|\theta _{i})|\psi _{j}),Y^{k}_{j}) \end{equation}

  It can be seen as the sum of each input modality’s average reconstruction error across all outputs.

  1. The latent represnteations from all input modalities should be close in the Euclidean sense.

  \begin{align} c_{2}(k|\boldsymbol \theta) = \frac {1}{|C||P|} \sum _{c \in C} \sum _{p\in P} var(f(X^{k}_{1}|\theta _{1})_{p,c},\ldots, f(X^{k}_{n}|\theta _{n})_{p,c})\!\!\!\notag \!\!\!\\ {}\end{align}


  1. The fused latent representation should produce all outputs as accurately as possible.

  \begin{align}&\hspace {-2pc}c_{3}(k|\boldsymbol \theta,\boldsymbol \psi) \notag \\=&\frac {1}{m}\sum _{j=1}^{m} MAE(g(\alpha (f(X^{k}_{1}|\theta _{1}), \ldots, f(X^{k}_{n}|\theta _{n}))|\psi _{j}), Y^{k}_{j})\notag \\ {}\end{align}

  The model is evaluated on the ISLES and BRATS datasets and demonstrate statistically significant improvements over state-of-the-art methods for single input tasks.
