---
title:  "[Reading] Deep learning, computer vision"
date:   2018-07-10 10:00:00
comments: true
#excerpt: ". "
tags:
  - annotated bibliography
---


1. B. Zoph and Q. V. Le, “Neural Architecture Search with Reinforcement Learning,” arXiv:1611.01578 [cs], Nov. 2016.

    **Reinforcement learning; hyperparameter optimization**

    Designing neural net architecture requires a lot of expert knowledge and ample time for trial-and-error. To improve this process of neural net design and hyperparameter searching, the researchers from Google applied reinforcement learning to this trial-and-error problem. They used a **RNN** as the **agent** that samples with probability *p* as **policy** to generate a string that specifies the network structure and connectivity. Thus, the generated CNN was trained on the real data and result in an **accuracy** as the **reward signal**. They then compute the **policy gradient** to update the **policy**. In this way, the controller leans to improve its search over time.

    In addition, the team applied the similar approach in the search of architecture with skip connections and other layer types such as pooling or batchnorm. It also used the reinforcement learning framework to generate recurrent cell architectures. The experiments showed that the model can compose novel network architecture that rivals the best human-invented architecture in terms of test set accuracy.


1. B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning Transferable Architectures for Scalable Image Recognition,” arXiv:1707.07012 [cs, stat], Jul. 2017.

    **Reinforcement learning, hyperparameter optimization**

    This work is from the same research group in Google as the above paper. It's a continuation to the above work of Neural Architecture Search (NAS). The NAS is computationally expensive since it will train multiple child networks on a large dataset. Therefore, the authors propose to search for architecture on a **proxy dataset**, and then transfer the learned architecture to a larger dataset. The main contribution of this work is the design of a novel search space, such that the best architecture found on the small dataset would scale to a larger dataset. It decouples the complexity of an architecture from the depth of a network, defines two building blocks (normal and reduction cells) which form the network by stacking alternately, and transferrers the same building block structure found on the small dataset to the large one by stacking more of the same building blocks. The structures of the building blocks are expressed with a sequence string and the best structure was searched with NAS framework on the small dataset. The resulting architectures approach or exceed state-of-the-art performance in both small and big dataset with less computation demand than human-designed architecture.



1. M. Zitnik and J. Leskovec, “Predicting multicellular function through multi-layer tissue networks,” Bioinformatics, vol. 33, no. 14, pp. i190–i198, Jul. 2017.

    **graph network**

    The aim of the paper is to model functions of proteins in specific human tissues in the protein-protein interaction network. Previous works ignore and could not differentiate the protein functions in different tissues. This paper present *OhmNet*, which learns features of protein functions in a protein-protein interaction network with an unsupervised learning fashion of node2vec embedding. It encourages sharing of similar features among proteins with similar network neighborhoods. To reserve the hierarchical relationship of proteins in different tissues, the model uses mathematical regularization to express the fact that the proteins "stay together functions similar". Specifically, it incorporates a recursive structure into the regularization that enforces the proteins to have similar features of the same proteins in the parent layer. The results showed *OhmNet* provided more accurate predictions of cellular function than alternative approaches, and also generated more accurate hypotheses about tissue-specific protein actions.




1. M. Haris, G. Shakhnarovich, and N. Ukita, “Deep Back-Projection Networks For Super-Resolution,” arXiv:1803.02735 [cs], Mar. 2018.


    **Super resolution**

    Image super-resolution task is to recover a high-resolution (HR) image from a low-resolution (LR) image. The current approach is to construct an HR image by learning non-linear LR-to-HR mapping, implemented as a deep neural network. Unlike the previous methods which predict the SR in a feed-forward manner, the author proposed Deep Back-Projection Networks that focus to directly increase the SR features using multiple up- and down- sampling stages, and feed the error predictions on each depth in the networks to revise the sampling results. Then, the model accumulates the self-correcting features from each upsampling stage to create SR image. The results show the effectiveness of the proposed network compares to other state-of-the-art methods.


1. X. Wang, K. Yu, C. Dong, and C. C. Loy, “Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform,” arXiv:1804.02815 [cs], Apr. 2018.

    **Super resolution**

    Similar to the above work, this is another image super-resolution works from CVPR 2018. It aims to recovery textures faithful to semantic classes. The categorical prior, which characterizes the semantic class of a region in an image (e.g.: sky, building, plant), is crucial for constraining the plausible solution space in SR. The authors use semantic segmentation maps as the categorical prior. To condition the network on semantic segmentations probability maps, the authors applied *Conditional Normalization* that applies a learned function of some conditions to replace parameters for feature-wise affine transformation in Batch normalization. To do that, they designed a Spatial Feature transform (SFT) layer. The prior is modeled by a pair of affine transformation parameters  $(\gamma, \beta)$. Then the transformation is carried out by applying the affine transformation to the feature maps of a specific layer. The SFT is modular and can be applied in between network layers. The general network architecture is a GAN. They applied perceptual loss and adversarial loss in the model. The experiments show that segmentation maps encapsulate rich categorical prior up to pixel level.

1. A. Dosovitskiy et al., “FlowNet: Learning Optical Flow with Convolutional Networks,” 2015, pp. 2758–2766.

    **FlowNet 1.0; optic flow**

    This work is on an end-to-end learning approach to estimating optical flow with CNN: given a dataset consisting of image pairs and ground truth flows field, a network is trained to predict the $x-y$ flow fields directly from the images. The optical flow estimation requires precise per-pixel localization, and finding correspondences between two input images. This paper is FlowNet 1.0. It proposed and compared two architectures: *FlowNetSimple* (FlowNetS) is a generic CNN. The authors simply stack two sequentially adjacent input images together and feed them through the network. *FlowNetCorr* (FlowNetC) first produces representations of the two images separately, then combines them together in the "correlation layer", and then learn the higher representation together. The correlation layer performs multiplicative patch comparisons between two feature maps. It uses a square patch to convolutes one feature map with another.    


    ![FlowNet Architecture](https://cdn-images-1.medium.com/max/800/0*XVygX0wF3enVQJLe.)

    Since the image resolution is reduced after a series of convolution and pooling layers, at the final refinement part, the authors refine the coarse feature map by unpooling and upconvolution. Then the feature maps are concatenated with corresponding feature maps from the contractive part of the network, and an upsampled coarse flow prediction. The "down- and up- sample" with "skip-connection" design preserves both the high-level information passed from coarser feature maps and fine local information provided in lower layer feature maps. To train the network, the authors developed a synthesis Flying Chairs dataset and the model achieves competitive accuracy at a frame rate of 5 to 10 fps.

1. E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox, “FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks,” arXiv:1612.01925 [cs], Dec. 2016.

    **FlowNet 2.0; optical flow**

    The FlowNet 1.0 resolves problems with small displacements and noisy artifacts in estimated flow fields. This paper built upon the previous FlowNet 1.0 and present three improvements: 1) the schedule of presenting data for training can help to decrease the error. 2) Since all state-of-the-art optical flow approaches rely on iterative methods, the authors hypotheses that deep network may also benefit from iterative refinement. To do this, they experiment with stacking multiple FlowNetS and FlowNetC. Subsequent networks get the input two adjacent images and the previous flow estimate as input. They used curriculum learning to train the stacked networks 3) the authors introduced a sub-network specializing on small motions. The experiments showed it performs on par with state-of-the-art methods, and computed optical flow at up to 140fps with accuracy matching the FlowNet 1.0.
