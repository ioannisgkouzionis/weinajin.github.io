---
title:  "[Reading] Deep Learning book"
date:   2018-07-18 10:00:00
comments: true
excerpt: "Reading notes of Deep Learning book by Ian Goodfellow, Yoshua Bengio and Aaron Courville"
tags:
  - annotated bibliography
---


# Chapter 9 Convolutional Networks

## 9.1 The convolution operation

To illustrate what is convolution operation, the authors give an example from 1-dimensional signal processing. The start point is to do a weighted average for a noisy signal. And it we apply such a weighted average function $w(a)$ to every point of the signal, we obtain a smoothed estimation of the signal. This operation is called **convolution**, i.e. each point in the output **feature map** is the summation of the weighted element-wise multiplication of the **input** and the **kernel**.

In the 2D imaging case, the kernel can be seen as a template of a part of an image. The convolution operation is a template matching process: the output will be bigger when the patterns in input match with the kernel. In my understanding, this is because at each point in the feature map, the operation is doing a dot point to compare the distance between the two patches.


## 9.3 pooling
Pooling helps to make the representation approximately **invariant** to small translations of the input, i.e. if we translate the input by a small amount, the values of most of the pooled outputs do not change. This is especially helpful if we care more about *whether some feature is present than exactly where it is*.

Note: in some applications such as segmentation, it is more important to preserve the location of a feature. In 9.4, it also mentioned a similar idea:

> If a task relies on preserving precise spatial information, then using pooling on all features can increase the training error. Some convolutional network architectures are designed to use pooling on some channels but not on other channels, in order to get both highly invariant features and features that will not underfit when the translation invariance prior is incorrect.

## 9.4 Convolution and pooling as an infinitely strong prior

Think CNN as a fully connected net with an infinitely strong prior. This prior says that the function the layer should learn contains only local interactions and is equivalent to translation and small translation (in the case of pooling layer).

Note: the structure of CNN gives prior constraints to the model before it has seen any data. The prior is based on the assumption for a specific task, such as in image classification, the detection of local features regardless of their variation is relatively crucial. 
